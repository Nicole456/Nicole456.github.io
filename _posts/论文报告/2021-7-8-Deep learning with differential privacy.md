---
layout: post
title: Deep learning with differential privacy
date: 2021-07-08 15:53:43
categories: 论文报告
tags: Deep-Learning  Privacy  Differential-Privacy
mathjax: true
---

| 论文名称     | Deep learning with differential privacy           |
| ------------ | ------------------------------------------------- |
| 论文作者     | M Abadi，A Chu，I Goodfellow，HB Mcmahan，L Zhang |
| 论文发表时间 | 2016/07/01                                        |
| 阅读时间     | 2021/07                                           |

## Abstract

在本论文中，提出将最先进的机器学习方法与先进的隐私保护机制相结合的策略，可以在适度的隐私预算下，并在软件复杂性、训练效率和模型质量方面以可接受的成本，来训练具有非凸目标的深度神经网络。









## Introduction

神经网络的进展在广泛的应用中取得了令人印象深刻的成功，这些进步的部分原因是有大量和代表性数据集来训练神经网络。这些数据集通常是众包的，并且可能包含敏感信息。它们的使用要求满足应用需求的技术，同时提供有原则的和严格的隐私保证。

在本论文中，我们将最先进的机器学习方法与先进的隐私保护机制相结合，在一个适度的（“单个位数”）隐私预算内训练神经网络。我们处理具有非凸目标、几层和数万到数百万个参数的模型。（相比之下，之前的工作在参数数量较少的凸模型，或者处理复杂但隐私损失较大的神经网络上获得了良好的结果。）为此，我们开发了新的算法技术，在差分隐私的框架内对隐私成本进行了重新定义的分析，以及谨慎的实现策略：

1. 证明通过跟踪**隐私损失的详细信息（更高的moments）**，可以获得，在渐近地和经验上地更严格的估计。
2.  通过引入新技术，**提高了差分隐私训练的计算效率**。这些技术包括计算单个训练样本梯度的有效算法，将任务细分为更小的批次以减少内存占用，以及在输入层应用差分隐私映射。
3.  将机器学习框架Tensor flow用于具有差分隐私的训练模型。在两个标准的图像分类任务MNIST和CIFAR-10上评估方法。经验表明，对深度神经网络的隐私保护可以在软件复杂性、训练效率和模型质量方面以适度的成本来实现。

机器学习系统通常包括有助于保护其训练数据的元素。特别是避免过度拟合训练的正则化技术。另一方面，解释深度神经网络中的内部表示是众所周知的困难的，但是它们的大容量意味着这些表示可能会编码至少一些训练数据的细节。在某些情况下，某些对手可能能够提取部分训练数据。

虽然模型反演攻击只需要“黑箱”访问训练过的模型（即通过输入和输出与模型交互），因此考虑具有额外能力的对手，即提供对抗完全了解训练机制和访问模型参数的强大对手的保护。此外当考虑对训练数据中的一条记录进行隐私保护，允许对手控制部分甚至全部其余训练数据，在实际中，这种可能性不能总是被排除在外，例如当数据是众包的时候。

## Background

### Differential Privacy

​		差分隐私构成了聚合数据库上算法隐私保证的强大标准

- #### Adjacent databases

  相邻数据集，给定两个数据集 $D$ 和 $D'$ , 若它们有且仅有一条数据不一样，则称此二者为相邻数据集。

- #### **Definition1** Differential Privacy

  假设存在一个随机函数$\mathcal{M}$ ，使得$\mathcal{M}$ 在任意两个相邻的数据集$D$和$D'$（即$\left \| D-D' \right \| _1\le 1$）上得到任意相同输出集合$\mathcal{S}$的概率满足
  
  $$Pr\left [ \mathcal{M} \left ( D \right ) \in S  \right ] \le e^{\varepsilon }  Pr\left [ \mathcal{M} \left ( D' \right ) \in S  \right ]+\delta  $$

  则称该随机函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BM%7D) 满足$\left ( \varepsilon ,\delta  \right ) $ -differential privacy（差分隐私），简写为$\left ( \varepsilon ,\delta  \right ) $ -DP。

- #### Gaussian noise mechanism

  >(补充差分隐私中的三种噪声机制)
  >
  >在应用差分隐私进行隐私保护中，需要处理的数据主要分为两大类，一类是`数值型`的数据；另外一类是`非数值型`的数据。
  >
  >- 对于`数值型`的数据，一般采用`Laplace`或者`高斯`机制，对得到数值结果加入随机噪声即可实现差分隐私；
  >- 而对于`非数值型`的数据，一般采用`指数`机制并引入一个`打分函数`，对每一种可能的输出都得到一个分数，归一化之后作为查询返回的概率值。
  
  Laplace机制提供的是严格的写为$\left ( \varepsilon ,0  \right ) $ -DP，而高斯机制则提供的是松弛的写为$\left ( \varepsilon ,\delta  \right ) $ -DP机制。
  
  $$\bigtriangleup f=\max_{D,D'} \left \| f(D)-f(D') \right \| _2$$
  
  Laplace定义的是$l_{1} $ ，这里的高斯定义的是$l_{2} $，这和两个分布的形式有很大关系，主要的目的也是为了方便差分隐私的证明。
  
  对于任意$\delta \in \left ( 0,1 \right ) $,$\sigma > \frac{\sqrt{2ln(1.25/\delta )}\bigtriangleup f }{\varepsilon } $，有噪声$Y\sim N(0,\sigma^2 )$满足$\left ( \varepsilon ,\delta  \right ) $ -DP
  
  $$Pr\left [ \mathcal{M} \left ( D \right ) \in S  \right ] \le e^{\varepsilon }  Pr\left [ \mathcal{M} \left ( D' \right ) \in S  \right ]+\delta  $$
  
  其中$\mathcal{M}(D)=f(D)+Y$，这里主要有三个参数，高斯分布的标准差 $\sigma$，这决定了噪声的尺度了；$\varepsilon$表示隐私预算，和噪声成负相关；$\delta $表示松弛项比如设置为$10^{-5}$，就表示只能容忍$10^{-5}$的概率违反严格差分隐私。高斯机制的证明在`privacy book`的附录A[[1\]](https://zhuanlan.zhihu.com/p/144318152#ref_1)里有

### Deep Learning

深度神经网络对于许多机器学习任务都非常有效，它将从输入到输出的参数化函数定义为许多层基本构建块的组成，如仿射变换和简单的非线性函数。后者的常用例子是sigmoids和rectifified linear units (ReLUs)。通过改变这些块的参数，我们可以“训练”这样一个参数化的函数，目的是拟合任何给定的有限的输入/输出示例集。

## Our Approach

差分隐私神经网络训练方法的主要组成部分：差分隐私随机梯度下降（SGD）算法、moments accountant和超参数调优

####  Differentially Private SGD Algorithm

算法1展示了差分隐私SGD算法的基本步骤，其目标函数通过不断训练和调整权重系数 $θ$来最 

小化损失函数$ \mathcal{L} $。

![image-20210712111646376](笔记.assets/image-20210712111646376.png)

在每次迭代过程中：

- 首先计算随机生成的批量样本的梯度，并基于计算生成的梯度值的$L_2$范数进行梯度剪切。

- 考虑到样本数据的隐私保护，基于附加高斯噪声方法以**梯度与随机噪声之和的均值**对剪切后的梯度进行更新，得到下一步迭代的权重系数$\theta $

- 最后除最终权重系数之外，还需要输出由于差分隐私保护机制带来的隐私损失。

  >- 梯度裁剪：由于差分隐私保护要求限制每个样本对于最终梯度$\tilde{g} _t$的影响，鉴于梯度的取值范围无先验 限定，故采用 Ｌ２ 范数首先对每个梯度进行裁剪，即梯度向量 $g $由 $g/\max (1,\left \| g \right \|_2/C )$替换，其中 $\mathcal{C} $为剪切阈值。裁剪后结果为：若$\left \| g \right \|_2\le C$，则保留 ｇ，若$\left \| g \right \|_2> C$， 则将其缩小为常量$ \mathcal{C} $。
  >
  >  >**神经网络的梯度裁剪Gradient Clipping**
  >  >
  >  >神经网络是通过梯度下降来学习的。而梯度爆炸问题一般会随着网络层数的增加而变得越来越明显。如果发生梯度爆炸，那么就是学过了，会直接跳过最优解。例如：在反向传播中，假设第一层倒数乘以权重> 1，随着向前网络的传播的层数越多，梯度可能会越来越大。 （梯度消失相反）所以需要梯度裁剪，避免模型越过最优点。
  >
  >- 神经网络各层参数：神经网络各层参数（即权重系数 $\theta $）都作为损失函数的$ \mathcal{L} $其中一部分参数。算法１同样表明，对于每一层而言均可以对剪切阈值和噪声程度进行单独设置，且可能随着训练迭代步骤ｔ的增 长而变化
  >
  >- Lot：和常规SGD类似，所提差分隐私SGD算法同样计算每次迭代过程中损失函数的梯度均值来估计$ \mathcal{L} $的梯度。该均值提供了一个无偏估计量(https://www.zhihu.com/question/22983179 )，且方差随着样本规模的扩大而迅速减小。为区别于常被称之为批处理的样本，称符合上述条件的样本为一个Lot。为限制样本数量占用内存的消耗，将批处理的样本规模设置得比Lot的规模小得多，其中 同样为所提差分隐私SGD的一个重要参数。随后，**将批处理的样本组合成为一个Lot进行噪声添加**。

####  The Moments Accountant

对于所提差分隐私SGD算法，除了确保算法运行的准确率以外，另一个重要的问题就是评估算法训练时的数据隐私损失成本。为此，提出**隐私损失累积函数**的概念来进行每次迭代过程访问训练数据的隐私损失以及随着训练进展时的累积隐私损失。

![image-20210712134946526](笔记.assets/image-20210712134946526.png)

不失一般性，令$\sigma = \frac{\sqrt{2log(1.25/\delta )}}{\varepsilon } $，文献[14]严格证明，对于抽样$q=\frac{\mathcal{L} }{N} $且$\varepsilon<1$，则对于完整样本而言，每次迭代过程都是$(O(q\varepsilon ),q\varepsilon )$差分隐私的。但文献并未对迭代过程以及噪声强度对差分隐私损失的影响展开研究，故无法对噪声强度以及剪切阈值$ \mathcal{C} $进行有依据的选取。故首先需要研究迭代过程对差分隐私的影响机制。

事实上，若令$\sigma \ge  c_2\frac{q\sqrt{Tlog(1/\delta )}}{\varepsilon } $，则同样应用文献[14]]方法，可以严格证明算法1对于任意的$\varepsilon < c_1q^2T$，都是$(O(q\varepsilon\sqrt{T} ),\delta )$-差分隐私的，其中$c_1和$$c_2$为常数。与文献[14]相比，本文的算法能够在相同迭代步骤 下，大幅度降低$\varepsilon $的数值，对数据的隐私性保护更高。

进一步的，对于两个相邻的数据集 $d$，$d'\in D$和映射机制$M$，引入一个辅助输入变量$\mathcal{aux} $和输出 ｏ∈Ｒ， 定义映射机制 Ｍ在输出$o\in R$处的隐私损失为：

$$$$

对于所提差分隐私 ＳＧＤ算法而言，神经网络各层权重系数的参数值与每次迭代过程中的差分隐私机制 

有着紧密的关联，从而对于给定的映射机制 Ｍ，在第 λ 次迭代过程的隐私损失定义为：

$$$$

进一步地，映射机制 Ｍ的损失边界值定义为：

$$$$

其满足如下特性：

1. 组合特性：给定一个机制 Ｍ，由一组子机制顺序组成，并满足从而总隐私损失边界满足：
2. 差分隐私边界：ε＞０，映射机制 Ｍ是（ε，δ） -差分隐私的，当且仅当

上述两条性质确定了深度盛景网克罗算法每次迭代的隐私损失以及能够达到侵犯数据隐私容忍度的最大迭代次数。特别的

[https://zhuanlan.zhihu.com/p/144318152]: https://zhuanlan.zhihu.com/p/144318152

