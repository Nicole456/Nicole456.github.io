---
layout: post
title: Unlearnable examples making personal data unexploitable
date: 2021-07-16 15:53:43
categories: 论文报告
tags: Deep-Learning  Privacy 
mathjax: true
---

## Abstract

互联网上大量的“免费”数据一直是当前深度学习成功的关键。然而，它也引起了人们对未经授权利用个人数据来训练商业模型的隐私方面担忧。因此，开发方法来防止利用未经授权的数据是至关重要的。这篇论文提出了一个问题：能够使得 数据对于深度学习模型来说是不可学习的吗？我们提出了一种误差最小化的噪声，它确实可以使训练样本无法学习。产生误差最小化噪声来将一个或多个训练样本的误差降到零，这可以欺骗模型相信从这些样本中无法学习什么东西。噪声被限制在人眼无法察觉的地方，因此不影响正常的数据效用。我们通过经验验证了样本形式和类形式的误差最小化噪声的有效性。在一个广泛的案例研究中，我们还证明了它在人脸识别实验环境下的灵活性和实用性。我们的工作为使个人数据不可利用于深度学习模型建立了重要的第一步。









## Introduction

近年来，深度学习在多个领域取得了突破性的成功，如计算机视觉和自然语言处理。这在一定程度上是由于从互联网上免费获取的大规模数据集，如ImageNet和ReCoRD。虽然这些数据集为开发深度学习模型提供了一个playground，但一个令人担忧的事实是，一些数据集是在未经相互同意的情况下收集的。个人数据也被无意识地从互联网上收集到，用于训练商业模型。这引起了公众对为未经授权甚至非法目的“自由”探索个人数据的担忧。

在本文中，我们通过引入不可学习的样本来解决这个问题，它旨在使训练的例子不可用于深度神经网络(DNNs)。换句话说，在不可学习的样本上训练的DNNs将具有相当于在正常测试样本上的随机猜测的性能。与通过混淆数据集中的信息来保护个人隐私相比，我们在这里实现的目标有所不同，但更具挑战性。首先，使得一个样本无法学习应该不会影响其正常使用。例如，一张无法学习的“自拍”照片应该没有明显的视觉缺陷，所以它可以被用作一张社交个人资料图片。理想情况下，这可以通过使用难以察觉的噪声来实现。在我们设置的情形中，噪声只能在模型训练前的一次场合（当数据上传到互联网时）添加到训练样本中。然而，DNNs对小噪声是稳健的或具备对抗性。目前还不清楚小的、难以察觉的噪声是否能阻止高性能DNNs的训练。

不可学习的样本的开发应该充分利用其独特的特征，更重要的是，DNNs的弱点。DNNs的一个被充分研究的特征是，它们倾向于捕获更多的数据的高频成分。令人惊讶的是，通过利用这一特性，我们发现当小的随机噪声以类的方式应用于训练数据时，可以很容易地欺骗DNNs过度拟合这种噪声。然而，early stopping可以有效地抵抗这类噪音。DNNs也很容易受到对抗性的（或误差最大限度的）噪声的影响，这些噪声是为在测试时使模型的误差最大化而设计的小扰动。我们发现，当以样本的方式应用于训练样本时，最大噪声不能停止DNN学习。这促使我们探索与误差最大化噪声的相反方向。具体来说，我们提出了一种误差最小化噪声，它可以防止模型在训练过程中受到目标函数的惩罚，从而欺骗模型相信无法从样本中学到什么。我们将一个包含误差最小化噪声的样本作为一个不可学习的样本。误差最小化噪声可以产生不同的形式：样本和类。类误差最小化噪声优于随机噪声，并且不能通过早期停止来规避。样本误差最小化噪声是唯一能使训练样本无法学习的有效噪声或误差最大化噪声。我们主要的贡献有:

- 我们提出了一种误差最小化噪声，它可以创建无法学习的示例，以防止个人数据被深度学习模型自由利用。噪声小，人眼不易察觉，因此不会降低大体上的数据利用率。
- 我们提出了一个双级优化过程，以有效地生成不同形式的误差最小化噪声：样本和类。
- 我们通过实验验证了误差最小化噪声对于创建不可学习的样本的有效性和灵活性。我们还通过一个人脸识别的案例来说明不可学习的例子在真实场景中的实际应用。

## Related work

在本节中，我们简要回顾了在数据隐私、数据中毒、对深度学习模型的对抗性攻击方面的大多数相关工作

**数据隐私。**隐私问题在隐私保护机器学习领域得到了广泛的研究。虽然这些工作在保护数据隐私方面取得了重大进展，但它们是基于这样的假设开发的，即模型可以自由地探索训练数据，并转向保护模型不泄露有关训练数据的敏感信息。在本文中，我们认为一个更具挑战性的情况下，防御者的目标是使对于未经授权的深度学习模型个人数据完全不可用。Fawkes（Shan et al.，2020）首次尝试了这种严格的情况。通过利用有针对性的对手攻击，Fawkes防止未经授权的面部追踪器跟踪一个人的身份。这项工作与我们的类似，因为我们有一个共同的目标，即防止未经授权的数据使用。与有针对性的对抗性攻击相比，我们提出了一种新的最小化错误噪声的方法来产生不可学习的样本，可以作为一个通用的框架用于广泛的数据保护任务。

**数据中毒**。数据中毒攻击的目的是通过修改训练样本来降低模型在干净样本上的性能。以前的工作已经证明了一种针对支持向量机的中毒攻击，该攻击利用对抗性（误差最大化）噪声对DNNs进行毒害最有影响力的训练样本，并且已经集成到端到端框架中。尽管数据中毒攻击可能会阻止随意的数据使用，但这些方法对DNNs的攻击非常有限，并且在实际场景中很难操作。例如，中毒的样本只会略微降低DNNs的性能，而且通常在清理样本时显得很容易区分，这会降低正常的数据实用性。后门（backdoor）攻击是另一种类型的攻击，它以隐形触发模式毒害训练数据。但是，后门攻击不会损害模型在干净数据上的性能。因此，它不是一种有效的数据保护方法。与这些工作不同的是，我们利用不可见噪声生成不可学习的样本来“绕过”DNNs的训练

**对抗性攻击**。研究发现，对抗性样本（或攻击）可以在测试时混淆DNNs 网络。对手发现一个错误最大化的噪声，使模型的预测误差最大化，并且可以为整个测试集通用地构造噪声。对抗性训练是对抗误差最大化噪声最有效的训练策略。对抗性训练可以归结为一个极小极大优化问题。在本文中，我们探索了误差最大化噪声的相反方向，即通过最小-最小优化过程找到使模型误差最小的小噪声。

## Unlearnable examples  and error-minimizing noise

### Problem Statement

对于defender能力的假设。我们假设defender可以完全访问他们想要使其无法学习的部分数据。但是，defender不能干扰训练过程，也不能访问完整的训练数据集。换言之，defender只能将其部分数据转换为无法学习的示例。此外，一旦创建了无法学习的示例，defender就不能进一步修改他们的数据。

**研究目标。**我们在DNNs图像分类的背景下提出了这个问题。给定了一个典型的k分类任务。

符号表示：

- 干净的训练数据集和测试数据集：$\mathcal{D}_{c}$，$\mathcal{D}_{t}$

- 无法学习的数据集：$\mathcal{D}_{u}=\left\{\left(\boldsymbol{x}_{i}', y_{i}\right)\right\}_{i=1}^{n}$，其中$x'=x+\delta$

- 噪声：$\delta$，通过$\|\boldsymbol{\delta}\|_{p} \leq \epsilon$以及$\left \| \cdot  \right \|_p $是$L_p$限界，$\epsilon $设置为很小的值，不会影响样本的正常效用。

- 问题表示，在典型情况下，DNN模型将在$\mathcal{D}_{c}$上训练，以学习从输入空间到标签空间的映射：$f：X→Y$。我们的目标是当在$\mathcal{D}_{u}$训练时，欺骗这个模型来学习噪声和标签之间的强相关性$f: \Delta \rightarrow \mathcal{Y}, \Delta \neq \mathcal{X}$

  $$\underset{\theta}{\arg \min } \mathbb{E}_{\left(\boldsymbol{x}^{\prime}, y\right) \sim \mathcal{D}_{u}} \mathcal{L}\left(f\left(\boldsymbol{x}^{\prime}\right), y\right)$$

- 其中，$\mathcal{L}$是分类损失，如常用的交叉熵损失

**噪声形式。**我们提出了两种形式的噪声：样本噪声和类噪声。对于样本噪声$\boldsymbol{x}_{i}^{\prime}=\boldsymbol{x}_{i}+\boldsymbol{\delta}_{i}, \boldsymbol{\delta}_{i} \in \Delta_{s}=\left\{\boldsymbol{\delta}_{1}, \cdots, \boldsymbol{\delta}_{n}\right\}$，对于类噪声$\boldsymbol{x}_{i}^{\prime}=\boldsymbol{x}_{i}+\boldsymbol{\delta}_{y_{i}}, \boldsymbol{\delta}_{y_{i}} \in \Delta_{c}=\left\{\boldsymbol{\delta}_{1}, \cdots, \boldsymbol{\delta}_{K}\right\}$，样本噪声需要为每个样本分别生成噪声。这可能有更有限的实用性。与样本噪声不同的是，类样本可以通过添加类噪声而变得不可学习，其中同一类中的所有示例都添加了相同的噪声。因此，在实际使用中可以更有效和更灵活地生成类噪声。然而，我们将看到类噪声可能更容易暴露。

### Generating error-minimizing noise

理想情况下，噪声应该在一个不同于$\mathcal{D}_{c}$的额外数据集上产生。这将涉及一个类匹配过程，以便从附加数据集中中为$\mathcal{D}_{c}$中要保护的每个类找到最合适的类。为了简单起见，我们在这里定义了在$\mathcal{D}_{c}$上噪声产生过程，并将验证在实验中使用额外数据集的有效性。给定一个干净的样本$x$，我们建议通过解决以下双级优化问题来生成训练输入$x$的误差最小化噪声δ：

$$\underset{\theta}{\arg \min } \mathbb{E}_{(\boldsymbol{x}, y) \sim \mathcal{D}_{c}}\left[\min _{\boldsymbol{\delta}} \mathcal{L}\left(f^{\prime}(\boldsymbol{x}+\boldsymbol{\delta}), y\right)\right] \quad \text { s.t. } \quad\|\boldsymbol{\delta}\|_{p} \leq \epsilon$$

其中$f'$表示用于产生噪声生成的源模型。注意这是最小-最小双级优化的问题。内部极小化是一个约束优化问题，它寻找$L_p$范数有界噪声$\delta$，使模型的分类损失最小化；外部极小化问题寻找参数θ，使模型的分类损失最小化。

请注意，上面的两级优化有两个部分优化同一个目标。为了找到有效的噪声$\delta$和不可学习的样本，与标准训练或对抗训练相比，$\theta $的优化步骤应该是有限的。具体地说，我们在每M步优化$\theta $之后优化$\mathcal{D}_{c}$上的δ。当错误率小于$\lambda $时，整个双层优化过程终止。

**按样本的生成。**我们采用了一阶优化方法PGD。解决受约束的内部最小化问题如下：

$$\boldsymbol{x}_{t+1}^{\prime}=\Pi_{\epsilon}\left(\boldsymbol{x}_{t}^{\prime}-\alpha \cdot \operatorname{sign}\left(\nabla_{\boldsymbol{x}} \mathcal{L}\left(f^{\prime}\left(\boldsymbol{x}_{t}^{\prime}\right), y\right)\right)\right)$$

其中

按类的生成。









## Experiments