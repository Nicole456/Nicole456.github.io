---
layout: post
title: Semi-supervised knowledge transfer for deep learning from private training data
date: 2021-07-10 15:53:43
categories: 论文报告
tags: Deep-Learning  Privacy  PATE
mathjax: true
---
| 论文名称     | 用半监督知识迁移解决深度学习中训练隐私数据的问题             |
| ------------ | ------------------------------------------------------------ |
| 论文作者     | N Papernot，M Abadi，Erlingsson, lfar，I Goodfellow，K Talwar |
| 论文发表时间 | 2016                                                         |
| 阅读时间     | 2021/07                                                      |

## Abstract

一些机器学习应用涉及在敏感的数据上进行训练,模型可能会无意间或隐式地存储其某些训练数据；因此，仔细分析模型可能会发现敏感信息。为了解决这个问题，该论文阐释了一种普遍适用的可以为训练数据提供强有力的隐私保证的方法：教师模型集成的隐私聚合（PATE）。该方法以黑盒的方式并行使用不相交的数据集（例如来自不同用户子集的记录）训练多个模型。因为它们直接依赖于敏感数据，所以这些模型没有公开，而是作为“学生”模型的“老师”。学生基于所有老师噪声投票的结果来学习预测输出，且无法直接访问单个老师或潜在数据或参数。就差分隐私而言，既可以直观地理解学生的隐私属性（因为没有一个老师，因此没有一个数据集可以指导学生的培训）。与以前的工作相比，该方法**仅对教师模型的训练方式作了较弱的假设**：它适用于任何模型，包括DNNs等非凸模型。











## Introduction

一些具有巨大益处的机器学习应用只有通过**分析敏感数据**，如用户的个人联系人、私人照片或通信，甚至医疗记录或遗传序列才能发挥作用。理想情况下，在这些情形中，学习算法将保护用户训练数据的隐私，例如，通过保证输出模型的生成不包括任何个体用户的细节。不幸的是，已建立的机器学习算法并未做出此类保证；事实上，虽然最先进的算法能够很好地推广到测试集，但在某种意义上它们持续性的过拟合特定的训练样本，因为这些样本是被其隐式记忆的。

 最近在利用机器学习中这种隐式记忆的攻击表明，私有的、敏感的训练数据可以从模型中恢复。这类攻击可以通过分析模型内部参数来直接进行，也可以通过反复访问查询不透明的模型来收集数据从而进行攻击的分析来间接进行。由于这些情况，**任何保护训练数据隐私的策略都应该谨慎地假设攻击者可以不受限制地访问内部模型参数，且隐私保证必须不仅适用于平均值，还应适用于最坏情况下的异常值**。

本文展示了如何**将学生模型训练限制在有限的教师投票范围内**，以及如何在仔细添加随机噪声后只显示最多的投票结果来加强该策略的隐私保障，称该加强的策略为PATE模型，意思是教师模型集成的隐私聚合。此外，还引入了一种改进的隐私分析，从而使该策略普遍适用于机器学习算法，从而使算法具有高可用性和意义的隐私保证，特别是当算法与半监督学习相结合时。

为了建立严格的隐私保障，限制学生模型对教师模型的访问，使学生模型对从教师模型的获得的知识可以进行有意义量化和界定是非常重要的。幸运的是，有许多技术可以加速知识迁移，从而降低学习过程中学生/老师的访问率。在本文中描述了几种技术，其中最有效的技术是将生成对抗网络（GANs）应用于半监督学习。

本论文的主要贡献如下：

- 展示了一种通用的机器学习策略，即PATE方法，该方法以“黑盒”的方式为训练数据提供了差分隐私，并独立于学习算法的方法策略
- 改进了用于保护训练数据隐私的学习模型的策略。 特别是，学生仅需获得教师的最高票，并且模型不需要经过受限的凸损失类训练。
- 我们探索了四种不同的方法来减少学生对老师的依赖，并展示了将 GAN 应用于半监督学习时， 如何通过从根本上减少对监督的需求来大大减少隐私损失。
- 提出了时刻计数技术的一种新应用，用于改进知识转移的差分隐私分析，从而可以训练具有有意义的隐私界限的学生。
- 评估了基于MNIST和SVHN的框架，从而可以将结果与以前的差分隐私机器学习方法进行比较。
- 证明了PATE方法可以成功地应用于其他模型结构以及具有不同特征的数据集。

## Private learning with ensembles of teachers

基于教师集成的隐私学习

![image-20210712152517842](G:\python projects\Nicole456.github.io\images\Semi-supervised knowledge transfer for deep learning from private training data\01.jpg)

方法概述：（1)教师集合训练在敏感数据的不相交子集上训练(2）学生模型使用集合标记的公共数据上训练。

### 训练教师模型

- **数据分区和教师：**数据集$(X,Y)$，其中$X$表示输入的集合，$Y$表示标签的集合，将数据划分为$n$个不相交的集合，并分别在每个集合上训练模型。假设在数据集大小和任务复杂度方面不太大，则获得了$n$个称为教师的分类器。然后，将它们部署为一个集合，通过查询每个老师对不可见输入$x$进行预测，并将它们聚合成一个单一的预测。

- **集成：**这个教师集成的隐私保障源于它的聚合。令$m$作为任务中的类的数量，给定类$j\in [m]$和输入$\vec{x}: n_{j}(\vec{x})=\left|\{i: i \in[n]\}, f_{i}(\vec{x})=j\right|$来计算分类为$j$的教师数量的标签计数。如果简单地应用相对多数，使用数量最大的标签，集成的决定可能取决于一个教师的投票。比如当两个标签的投票票数最多相差一个时，就有一个平局：如果一个老师做出不同的预测，聚合的输出就会变化。因此对投票计数增加了随机噪声，以引入不确定性：

  $$f(x)=\arg \max _{j}\left\{n_{j}(\vec{x})+\operatorname{Lap}\left(\frac{1}{\gamma}\right)\right\}$$

  在等式中，$\gamma$是一个隐私参数，$Lap(b)$为位置0和尺度为b的拉普拉斯分布。$\gamma$参数会影响可证明的隐私保证。直观地讲，较大的$\gamma$会导致较强的隐私保证，但会降低标签的准确性，因为上述最大含有噪声的$f$可能与实际的最大数不同。

  虽然我们可以使用上述的$f$来进行预测，但随着进行更多的预测，所需的噪声也会增加，从而使模型在无数次查询后变得无法使用。此外，当攻击者有权访问模型参数时，隐私保证将不成立。实际上，由于每位教师都在不考虑隐私的情况下接受了训练，因此可以想象他们具有足够的能力来保留训练数据的详细信息。为了解决这些局限性，考虑使用教师集体预测的固定数量的标签来训练另一种模型，即学生。

### 从集合到学生的知识半监督迁移

使用非敏感和未标记数据训练学生，其中一些数据是使用集成机制标记的。此学生模型是一种被部署的模型，代替了教师的集合，**以便将隐私损失固定为一个值**，**该值不随对学生模型进行的查询的次数而增长**。实际上，隐私损失现在由在学生训练期间对教师进行的查询数量决定，并且不会随着最终用户查询已部署的学生模型而增加。 因此，即使学生的架构和参数是公开的或者由攻击者进行逆向工程，也可以保留贡献原始训练数据集的用户的隐私。

通过GANs训练学生：GAN框架包括两个机器学习模型，一个生成器和一个判别器。他们以一种对抗的方式进行训练，这可以被视为一种二人零和博弈。生成器通过转换从高斯分布中采样的向量从数据分布中产生采样。判别器经过训练，可以将生成器生成的样本与实际数据分布的样本区分开。通过同时梯度下降降低两个参与者损失函数来训练模型。在实践中，当策略集是非凸的（例如DNN）时，通常很难控制这些动态过程。 Salimans等人（2016）在将GAN应用于半监督学习中进行了以下修改。判别器从二分类器（数据与生成器样本）扩展到多分类器（k个数据样本类别之一，再加上一个生成样本的类别）。 然后训练该分类器以将正确类别中的标记的真实样本，k个类别中的任何一个中的未标记的真实样本，并对多出类别的生成样本进行分类。

尽管目前尚无正式结果可以解释其原因，但已通过经验证明了该技术可大大改善多个数据集上分类器的半监督学习，尤其是在分类器增加特征匹配损失情况下训练。

以半监督方式训练学生可以更好地利用学生可用的全部数据，并仅标记其中的一部分。在无监督的学习中使用未标记的输入来估计先验分布。之后标记的输入用于有监督学习。

## Privacy analysis of the approach 

### 差分隐私的准备工作和PATE的简单分析

### 时刻统计：一个更好的分析的基石

### PATE精确的，数据依赖的隐私分析